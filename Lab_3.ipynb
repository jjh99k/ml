{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjh99k/ml/blob/main/Lab_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC91mF_u6o-s"
      },
      "source": [
        "# Machine Learning - Lab Session 3\n",
        "\n",
        "This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. \n",
        "\n",
        "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
        "\n",
        "### Submitting your work:\n",
        "- <font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.\n",
        "- Commit the `.ipynb` file on your github repository and submit URL on E-Ruri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5bL80Gdm-7TI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C62c6fKX98aM"
      },
      "source": [
        "## Preparing Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnm87-pp6o-v"
      },
      "source": [
        "### Load MNIST Data\n",
        "We temperaly use `sklearng.datasets` for load MNIST before we learning `pytorch`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "both",
        "id": "JLpLa8Jt7Vu4"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
        "X = X / 255.0\n",
        "y = y.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "pv_1sPkqByzq",
        "outputId": "2f09f6b5-0832-4597-97ea-f64432594866"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAN9klEQVR4nO3df4xV9ZnH8c+zWP6QojBrOhKKSyEGg8ZON4gbl6w1hvojGhw1TSexoZE4/YNJaLIhNewf1WwwZBU2SzTNTKMWNl1qEzUgaQouoOzGhDgiKo5LdQ2mTEaowZEf/mCHefaPezBTnfu9w7nn3nOZ5/1Kbu6957nnnicnfDi/7pmvubsATH5/VXYDAJqDsANBEHYgCMIOBEHYgSAuaubCzIxT/0CDubuNN72uLbuZ3Wpmh8zsPTN7sJ7vAtBYlvc6u5lNkfRHSUslHZH0qqQudx9IzMOWHWiwRmzZF0t6z93fd/czkn4raVkd3weggeoJ+2xJfxrz/kg27S+YWbeZ9ZtZfx3LAlCnhp+gc/c+SX0Su/FAmerZsg9KmjPm/bezaQBaUD1hf1XSlWb2HTObKulHkrYV0xaAouXejXf3ETPrkbRD0hRJT7n724V1BqBQuS+95VoYx+xAwzXkRzUALhyEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBJF7yGZcGKZMmZKsX3rppQ1dfk9PT9XaxRdfnJx3wYIFyfrKlSuT9ccee6xqraurKznv559/nqyvW7cuWX/44YeT9TLUFXYzOyzppKSzkkbcfVERTQEoXhFb9pvc/aMCvgdAA3HMDgRRb9hd0k4ze83Musf7gJl1m1m/mfXXuSwAdah3N36Juw+a2bckvWhm/+Pue8d+wN37JPVJkpl5ncsDkFNdW3Z3H8yej0l6XtLiIpoCULzcYTezaWY2/dxrST+QdLCoxgAUq57d+HZJz5vZue/5D3f/QyFdTTJXXHFFsj516tRk/YYbbkjWlyxZUrU2Y8aM5Lz33HNPsl6mI0eOJOsbN25M1js7O6vWTp48mZz3jTfeSNZffvnlZL0V5Q67u78v6bsF9gKggbj0BgRB2IEgCDsQBGEHgiDsQBDm3rwftU3WX9B1dHQk67t3707WG32baasaHR1N1u+///5k/dSpU7mXPTQ0lKx//PHHyfqhQ4dyL7vR3N3Gm86WHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeC4Dp7Adra2pL1ffv2Jevz5s0rsp1C1ep9eHg4Wb/pppuq1s6cOZOcN+rvD+rFdXYgOMIOBEHYgSAIOxAEYQeCIOxAEIQdCIIhmwtw/PjxZH316tXJ+h133JGsv/7668l6rT+pnHLgwIFkfenSpcn66dOnk/Wrr766am3VqlXJeVEstuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EAT3s7eASy65JFmvNbxwb29v1dqKFSuS8953333J+pYtW5J1tJ7c97Ob2VNmdszMDo6Z1mZmL5rZu9nzzCKbBVC8iezG/1rSrV+Z9qCkXe5+paRd2XsALaxm2N19r6Sv/h50maRN2etNku4quC8ABcv72/h2dz83WNaHktqrfdDMuiV151wOgILUfSOMu3vqxJu790nqkzhBB5Qp76W3o2Y2S5Ky52PFtQSgEfKGfZuk5dnr5ZK2FtMOgEapuRtvZlskfV/SZWZ2RNIvJK2T9DszWyHpA0k/bGSTk92JEyfqmv+TTz7JPe8DDzyQrD/zzDPJeq0x1tE6aobd3buqlG4uuBcADcTPZYEgCDsQBGEHgiDsQBCEHQiCW1wngWnTplWtvfDCC8l5b7zxxmT9tttuS9Z37tyZrKP5GLIZCI6wA0EQdiAIwg4EQdiBIAg7EARhB4LgOvskN3/+/GR9//79yfrw8HCyvmfPnmS9v7+/au2JJ55IztvMf5uTCdfZgeAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIrrMH19nZmaw//fTTyfr06dNzL3vNmjXJ+ubNm5P1oaGhZD0qrrMDwRF2IAjCDgRB2IEgCDsQBGEHgiDsQBBcZ0fSNddck6xv2LAhWb/55vyD/fb29ibra9euTdYHBwdzL/tClvs6u5k9ZWbHzOzgmGkPmdmgmR3IHrcX2SyA4k1kN/7Xkm4dZ/q/untH9vh9sW0BKFrNsLv7XknHm9ALgAaq5wRdj5m9me3mz6z2ITPrNrN+M6v+x8gANFzesP9S0nxJHZKGJK2v9kF373P3Re6+KOeyABQgV9jd/ai7n3X3UUm/krS42LYAFC1X2M1s1pi3nZIOVvssgNZQ8zq7mW2R9H1Jl0k6KukX2fsOSS7psKSfunvNm4u5zj75zJgxI1m/8847q9Zq3StvNu7l4i/t3r07WV+6dGmyPllVu85+0QRm7Bpn8pN1dwSgqfi5LBAEYQeCIOxAEIQdCIKwA0FwiytK88UXXyTrF12Uvlg0MjKSrN9yyy1Vay+99FJy3gsZf0oaCI6wA0EQdiAIwg4EQdiBIAg7EARhB4KoedcbYrv22muT9XvvvTdZv+6666rWal1Hr2VgYCBZ37t3b13fP9mwZQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBILjOPsktWLAgWe/p6UnW77777mT98ssvP++eJurs2bPJ+tBQ+q+Xj46OFtnOBY8tOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EwXX2C0Cta9ldXeMNtFtR6zr63Llz87RUiP7+/mR97dq1yfq2bduKbGfSq7llN7M5ZrbHzAbM7G0zW5VNbzOzF83s3ex5ZuPbBZDXRHbjRyT9o7svlPR3klaa2UJJD0ra5e5XStqVvQfQomqG3d2H3H1/9vqkpHckzZa0TNKm7GObJN3VqCYB1O+8jtnNbK6k70naJ6nd3c/9OPlDSe1V5umW1J2/RQBFmPDZeDP7pqRnJf3M3U+MrXlldMhxB2109z53X+Tui+rqFEBdJhR2M/uGKkH/jbs/l00+amazsvosScca0yKAItTcjTczk/SkpHfcfcOY0jZJyyWty563NqTDSaC9fdwjnC8tXLgwWX/88ceT9auuuuq8eyrKvn37kvVHH320am3r1vQ/GW5RLdZEjtn/XtKPJb1lZgeyaWtUCfnvzGyFpA8k/bAxLQIoQs2wu/t/Sxp3cHdJNxfbDoBG4eeyQBCEHQiCsANBEHYgCMIOBMEtrhPU1tZWtdbb25uct6OjI1mfN29erp6K8MorryTr69evT9Z37NiRrH/22Wfn3RMagy07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgQR5jr79ddfn6yvXr06WV+8eHHV2uzZs3P1VJRPP/20am3jxo3JeR955JFk/fTp07l6Quthyw4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQYS5zt7Z2VlXvR4DAwPJ+vbt25P1kZGRZD11z/nw8HByXsTBlh0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgjB3T3/AbI6kzZLaJbmkPnf/NzN7SNIDkv6cfXSNu/++xnelFwagbu4+7qjLEwn7LEmz3H2/mU2X9Jqku1QZj/2Uuz820SYIO9B41cI+kfHZhyQNZa9Pmtk7ksr90ywAztt5HbOb2VxJ35O0L5vUY2ZvmtlTZjazyjzdZtZvZv11dQqgLjV347/8oNk3Jb0saa27P2dm7ZI+UuU4/p9V2dW/v8Z3sBsPNFjuY3ZJMrNvSNouaYe7bxinPlfSdne/psb3EHagwaqFveZuvJmZpCclvTM26NmJu3M6JR2st0kAjTORs/FLJP2XpLckjWaT10jqktShym78YUk/zU7mpb6LLTvQYHXtxheFsAONl3s3HsDkQNiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQii2UM2fyTpgzHvL8umtaJW7a1V+5LoLa8ie/ubaoWm3s/+tYWb9bv7otIaSGjV3lq1L4ne8mpWb+zGA0EQdiCIssPeV/LyU1q1t1btS6K3vJrSW6nH7ACap+wtO4AmIexAEKWE3cxuNbNDZvaemT1YRg/VmNlhM3vLzA6UPT5dNobeMTM7OGZam5m9aGbvZs/jjrFXUm8Pmdlgtu4OmNntJfU2x8z2mNmAmb1tZquy6aWuu0RfTVlvTT9mN7Mpkv4oaamkI5JeldTl7gNNbaQKMzssaZG7l/4DDDP7B0mnJG0+N7SWmf2LpOPuvi77j3Kmu/+8RXp7SOc5jHeDeqs2zPhPVOK6K3L48zzK2LIvlvSeu7/v7mck/VbSshL6aHnuvlfS8a9MXiZpU/Z6kyr/WJquSm8twd2H3H1/9vqkpHPDjJe67hJ9NUUZYZ8t6U9j3h9Ra4337pJ2mtlrZtZddjPjaB8zzNaHktrLbGYcNYfxbqavDDPeMusuz/Dn9eIE3dctcfe/lXSbpJXZ7mpL8soxWCtdO/2lpPmqjAE4JGl9mc1kw4w/K+ln7n5ibK3MdTdOX01Zb2WEfVDSnDHvv51NawnuPpg9H5P0vCqHHa3k6LkRdLPnYyX38yV3P+ruZ919VNKvVOK6y4YZf1bSb9z9uWxy6etuvL6atd7KCPurkq40s++Y2VRJP5K0rYQ+vsbMpmUnTmRm0yT9QK03FPU2Scuz18slbS2xl7/QKsN4VxtmXCWvu9KHP3f3pj8k3a7KGfn/lfRPZfRQpa95kt7IHm+X3ZukLars1v2fKuc2Vkj6a0m7JL0r6T8ltbVQb/+uytDeb6oSrFkl9bZElV30NyUdyB63l73uEn01Zb3xc1kgCE7QAUEQdiAIwg4EQdiBIAg7EARhB4Ig7EAQ/w8ie3GmjcGk5QAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.imshow(X[0].reshape((28, 28)), cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7aHrm6nGDMB"
      },
      "source": [
        "### Preprocessing\n",
        "Reformat into a shape that's more adapted to the models we're going to train:\n",
        "1. labels as float 1-hot encodings. [hint, use [`np.eye`](https://numpy.org/devdocs/reference/generated/numpy.eye.html)] **[pts. 5]**\n",
        "2. split the data into train and test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7TnyPMF_CuM",
        "outputId": "162c59af-1f27-4604-ac80-0c12e73b602b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before encoding: 5\n",
            "after encoding: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# one-hot encoding\n",
        "print('before encoding: %s'% y[0])\n",
        "y = np.eye(10)[y]\n",
        "print('after encoding: %s' % y[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRSyYiIIGIzS",
        "outputId": "bfacb2bc-3ab1-4f9e-c99e-bc0a14d84541"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set (56000, 784) (56000, 10)\n",
            "Test set (14000, 784) (14000, 10)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into train partition and test partition\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, random_state=0, test_size=0.2)\n",
        "\n",
        "IMAGE_SIZE = 28\n",
        "NUM_LABELS = 10\n",
        "\n",
        "print('Training set', X_train.shape, y_train.shape)\n",
        "print('Test set', X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lw7LP8x_OWi"
      },
      "source": [
        "In this lab session, we only use partial data for training and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jNaDkIi46o-y"
      },
      "outputs": [],
      "source": [
        "DATA_SIZE = 100\n",
        "\n",
        "X_train = X_train[0:DATA_SIZE]\n",
        "y_train = y_train[0:DATA_SIZE]\n",
        "\n",
        "X_test = X_test[0:DATA_SIZE]\n",
        "y_test = y_test[0:DATA_SIZE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu-bNfMf6o-z"
      },
      "source": [
        "## Training a Neural Networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewBirlzP6o-1"
      },
      "source": [
        "### Activation Function\n",
        "\n",
        "First let's implement two activation function, `sigmoid` and `softmax`. Also the derivation of `sigmoid` function. **[pts. 5 per function]**\n",
        "$$\n",
        "\\begin{align}\n",
        "\\sigma(x) &=  \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-x} } \\\\\n",
        "\\mathrm{softmax}(x_{i}) &= \\frac{e^{x_i}}{\\sum_j e^{x_j}} \\\\\n",
        "\\sigma'(x) &= \\sigma(x)\\sigma(1-x) \\\\\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "m_9pl55P6o-1"
      },
      "outputs": [],
      "source": [
        "def sigmoid(Z):\n",
        "    return 1 / (1+np.exp(-Z))\n",
        "\n",
        "def softmax(Z):\n",
        "    return np.exp(Z) / np.exp(Z).sum(axis = 0, keepdims=True)\n",
        "\n",
        "def sigmoid_derivative(Z):\n",
        "    s = 1 / (1+np.exp(-Z))\n",
        "    return s * (1-s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpeOlzYaAjVU",
        "outputId": "43bf051d-8848-428b-da0d-7ff2ce4f052a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.52497918747894\n",
            "[0.26175419 0.50140083 0.23684498]\n",
            "0.24937604019289197\n"
          ]
        }
      ],
      "source": [
        "# test functions\n",
        "print(sigmoid(0.1))\n",
        "print(softmax([0.15, 0.8, 0.05]))\n",
        "print(sigmoid_derivative(0.1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH3YKCDC6o-z"
      },
      "source": [
        "Let's now build a neural network with one input layer, one hidden layer, and one output layer. The number of nodes in the input layer is determined by the dimensionality of our data, 784. Similarly, the number of nodes in the output layer is determined by the number of classes we have, 10. The input to the network will be the pixel values of the input image and its output will be ten probabilities, ones for each class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B1bQjCm_xkL"
      },
      "source": [
        "![Sample network](https://drive.google.com/uc?id=1D1XuhvNokK5S5yn8V34JGvyE-XXz9rST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObPwuWqv6o-z"
      },
      "source": [
        "### How our network makes predictions\n",
        "\n",
        "Our network makes predictions using *forward propagation*, which is just a bunch of matrix multiplications and the application of the activation function(s) we defined above. If $x$ is the 784-dimensional input to our network then we calculate our prediction $\\hat{y}$ (ten-dimensional) as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJBKF3zf6o-z"
      },
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{s}_1 & = W_1\\mathbf{x} + \\mathbf{b}_1 \\\\\n",
        "\\mathbf{z}_1 & = \\mathrm{sigmoid}(\\mathbf{s}_1) \\\\\n",
        "\\mathbf{s}_2 & = W_2\\mathbf{z}_1 + \\mathbf{b}_2 \\\\\n",
        "\\mathbf{h} & = \\hat{y} = \\mathrm{softmax}(\\mathbf{s}_2)\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG3J5N6A6o-0"
      },
      "source": [
        "$z_i$ is the input of layer $i$ and $a_i$ is the output of layer $i$ after applying the activation function. $W_1, b_1, W_2, b_2$ are  parameters of our network, which we need to learn from our training data. You can think of them as matrices transforming data between layers of the network. Looking at the matrix multiplications above we can figure out the dimensionality of these matrices.\n",
        "\n",
        "If we use 1024 nodes for our hidden layer then $W_1 \\in \\mathbb{R}^{1024\\times784}$, $b_1 \\in \\mathbb{R}^{1024\\times1}$, $W_2 \\in \\mathbb{R}^{10\\times1024}$, $b_2 \\in \\mathbb{R}^{10\\times1}$. Now you see why we have more parameters if we increase the size of the hidden layer.\n",
        "\n",
        "**[note]** beware of dimension of $\\mathbf{x}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pVFqhiNb_prh"
      },
      "outputs": [],
      "source": [
        "layers_size = [784, 50, 10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDjwqr-uIXM6"
      },
      "source": [
        "Define parameters and initialize them with `np.random.rand` for W and `np.zeros` for b. **[pts. 5]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayJva00z_oB2",
        "outputId": "ec1f753c-3f24-4b6c-d61b-771e15f599c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 784)\n",
            "(50, 1)\n",
            "(10, 50)\n",
            "(10, 1)\n"
          ]
        }
      ],
      "source": [
        "# initialization\n",
        "np.random.seed(1)\n",
        "\n",
        "parameters = {}\n",
        "parameters[\"W1\"] = np.random.randn(layers_size[1], layers_size[0]) / np.sqrt(layers_size[0])\n",
        "parameters[\"b1\"] = np.zeros((layers_size[1], 1))\n",
        "\n",
        "parameters[\"W2\"] = np.random.randn(layers_size[2], layers_size[1]) / np.sqrt(layers_size[1])\n",
        "parameters[\"b2\"] = np.zeros((layers_size[2], 1))\n",
        "\n",
        "print(parameters[\"W1\"].shape)\n",
        "print(parameters[\"b1\"].shape)\n",
        "print(parameters[\"W2\"].shape)\n",
        "print(parameters[\"b2\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feed forward $\\mathbf{x}$ through Networks. **[pts. 10]**"
      ],
      "metadata": {
        "id": "lorKSUwDYoNZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vD-7B0fknV2",
        "outputId": "2dab3c9f-711c-4eeb-a2c6-fa8014f91282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 100)\n",
            "(10, 100)\n"
          ]
        }
      ],
      "source": [
        "# forward\n",
        "store = {}\n",
        "store['X'] = X_train.T\n",
        "\n",
        "## input to hidden\n",
        "S = parameters[\"W1\"].dot(store['X']) + parameters[\"b1\"]\n",
        "Z = sigmoid(S)\n",
        "store[\"Z\"] = Z\n",
        "store[\"W1\"] = parameters[\"W1\"]\n",
        "store[\"S1\"] = S\n",
        "\n",
        "## hidden to output\n",
        "S = parameters[\"W2\"].dot(Z) + parameters[\"b2\"]\n",
        "H = softmax(S)\n",
        "store[\"H\"] = H        \n",
        "store[\"W2\"] = parameters[\"W2\"]\n",
        "store[\"S2\"] = S      \n",
        "\n",
        "print(store[\"Z\"].shape)\n",
        "print(store[\"H\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtkfchpT6o-0"
      },
      "source": [
        "### Learning the Parameters\n",
        "\n",
        "Learning the parameters for our network means finding parameters ($W_1, b_1, W_2, b_2$) that minimize the error on our training data. But how do we define the error? We call the function that measures our error the *loss function*. A common choice with the softmax output is the [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression). If we have $N$ training examples and $C$ classes then the loss for our prediction $\\hat{y}$ with respect to the true labels $y$ is given by:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "L(y,\\hat{y}) = - \\frac{1}{N} \\sum_{n \\in N} \\sum_{i \\in C} y_{n,i} \\log\\hat{y}_{n,i}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIiOvIyZ6o-0"
      },
      "source": [
        "The formula looks complicated, but all it really does is sum over our training examples and add to the loss if we predicted the incorrect class. So, the further away $y$ (the correct labels) and $\\hat{y}$ (our predictions) are, the greater our loss will be. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldCES_Px6o-0"
      },
      "source": [
        "Remember that our goal is to find the parameters that minimize our loss function. We can use [gradient descent](http://cs231n.github.io/optimization-1/) to find its minimum. I will implement the most vanilla version of gradient descent, also called batch gradient descent with a fixed learning rate. Variations such as SGD (stochastic gradient descent) or minibatch gradient descent typically perform better in practice. So if you are serious you'll want to use one of these, and ideally you would also [decay the learning rate over time](http://cs231n.github.io/neural-networks-3/#anneal).\n",
        "\n",
        "As an input, gradient descent needs the gradients (vector of derivatives) of the loss function with respect to our parameters: $\\frac{\\partial{L}}{\\partial{W_1}}$, $\\frac{\\partial{L}}{\\partial{b_1}}$, $\\frac{\\partial{L}}{\\partial{W_2}}$, $\\frac{\\partial{L}}{\\partial{b_2}}$. To calculate these gradients we use the famous *backpropagation algorithm*, which is a way to efficiently calculate the gradients starting from the output. I won't go into detail how backpropagation works, but there are many excellent explanations ([here](http://colah.github.io/posts/2015-08-Backprop/) or [here](http://cs231n.github.io/optimization-2/)) floating around the web."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMCgWwWb6o-0"
      },
      "source": [
        "Applying the backpropagation formula we find the following (trust me on this): **[pts. 20]**\n",
        "\n",
        "backpropagation for output layer\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\mathbf{e} = \\hat{y} - y \\\\\n",
        "& \\delta_2 = \\mathbf{e} \\sigma'(\\mathbf{s}_2) \\\\\n",
        "& \\frac{\\partial{L}}{\\partial{W_2}} = \\mathbf{z}^T \\delta_2  \\\\\n",
        "& \\frac{\\partial{L}}{\\partial{b_2}} = \\delta_2 \\\\ \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "backpropagation for hidden layer\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\delta_1 = W_2^T\\delta_2 \\circ \\sigma'(\\mathbf{s}_1)  \\\\\n",
        "& \\frac{\\partial{L}}{\\partial{W_1}} = \\mathbf{x}^T \\delta_1\\\\\n",
        "& \\frac{\\partial{L}}{\\partial{b_1}} = \\delta_1 \\\\\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJnCUVgepcx0",
        "outputId": "719eccd2-5ca8-4af1-ba0a-49f6f853f1c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 784)\n",
            "(50, 1)\n",
            "(10, 50)\n",
            "(10, 1)\n"
          ]
        }
      ],
      "source": [
        "n = X_train.shape[0]\n",
        "derivatives = {}\n",
        "\n",
        "# backward\n",
        "## output to hidden\n",
        "error = store[\"H\"] - y_train.T\n",
        "deleta2 = error * sigmoid_derivative(store[\"S2\"])\n",
        "\n",
        "dW = deleta2.dot(store[\"Z\"].T)\n",
        "db = np.sum(deleta2, axis=1, keepdims=True)\n",
        "\n",
        "derivatives[\"dW2\"] = dW\n",
        "derivatives[\"db2\"] = db\n",
        "\n",
        "## hidden to intput\n",
        "dAPrev = store[\"W2\"].T.dot(deleta2)\n",
        "deleta1 = dAPrev * sigmoid_derivative(store[\"S1\"])\n",
        "\n",
        "dW = deleta1.dot(store[\"X\"].T)\n",
        "db = np.sum(deleta1, axis=1, keepdims=True)\n",
        "\n",
        "derivatives[\"dW1\"] = dW\n",
        "derivatives[\"db1\"] = db\n",
        "\n",
        "print(derivatives['dW1'].shape)\n",
        "print(derivatives['db1'].shape)\n",
        "print(derivatives['dW2'].shape)\n",
        "print(derivatives['db2'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jw22p14wap8"
      },
      "source": [
        "### Gradient Descent\n",
        "Update via gradient descent as follows: **[pts. 10]**\n",
        "$$\n",
        "\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta ∇\\frac{\\partial \\varepsilon}{\\partial\\mathbf{w}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hkt5YtKAu_Zd"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "\n",
        "parameters[\"W2\"] = parameters[\"W2\"] - learning_rate * derivatives[\"dW2\"]\n",
        "parameters[\"b2\"] = parameters[\"b2\"] - learning_rate * derivatives[\"db2\"]\n",
        "\n",
        "parameters[\"W1\"] = parameters[\"W1\"] - learning_rate * derivatives[\"dW1\"]\n",
        "parameters[\"b1\"] = parameters[\"b1\"] - learning_rate * derivatives[\"db1\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjXVLPGE6o-0"
      },
      "source": [
        "## Implementation of Whole Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pUWHI8x6o-1"
      },
      "source": [
        "Finally, here comes the function to train our Artificial Neural Network as class. Complete the code. **[pts. 30]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "-nIqBuT66o-1"
      },
      "outputs": [],
      "source": [
        "class ANN:\n",
        "    def __init__(self, layers_size):\n",
        "        self.layers_size = layers_size\n",
        "        self.parameters = {}\n",
        "        self.L = len(self.layers_size)\n",
        "        self.n = 0\n",
        "        self.costs = []\n",
        "        self.val_costs = []\n",
        " \n",
        "    def sigmoid(self, Z):\n",
        "        return sigmoid(Z)\n",
        " \n",
        "    def softmax(self, Z):\n",
        "        return softmax(Z)\n",
        "        ##return ###################################\n",
        "\n",
        "    def sigmoid_derivative(self, Z):\n",
        "        return sigmoid_derivative(Z)\n",
        "        ##return #############\n",
        " \n",
        "    def initialize_parameters(self):\n",
        "        np.random.seed(1)\n",
        "        ########### complete the code ################\n",
        "        self.parameters[\"W1\"] = parameters[\"W1\"]\n",
        "        self.parameters[\"b1\"] = parameters[\"b1\"]\n",
        "\n",
        "        self.parameters[\"W2\"] = parameters[\"W2\"]\n",
        "        self.parameters[\"b2\"] = parameters[\"b2\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##############################################\n",
        "\n",
        "    def forward(self, X):\n",
        "        store = {}\n",
        "        store['X'] = X.T\n",
        "        \n",
        "        ########### complete the code ################\n",
        "        S = self.parameters[\"W1\"].dot(store['X']) + self.parameters[\"b1\"]\n",
        "        Z = self.sigmoid(S)\n",
        "        store[\"Z\"] = Z\n",
        "        store[\"W1\"] = self.parameters[\"W1\"]\n",
        "        store[\"S1\"] = S\n",
        "\n",
        "        S = self.parameters[\"W2\"].dot(store[\"Z\"]) + self.parameters[\"b2\"]\n",
        "        H = self.softmax(S)\n",
        "        store[\"H\"] = Z\n",
        "        store[\"W2\"] = self.parameters[\"W2\"]\n",
        "        store[\"S2\"] = S\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##############################################\n",
        "\n",
        "        return H, store\n",
        " \n",
        "    def backward(self, X, Y, store):\n",
        "        ########### complete the code ################\n",
        "        derivatives = {}\n",
        "\n",
        "        error = store[\"H\"] - Y.T\n",
        "        deleta2 = error * self.sigmoid_derivative(store[\"S2\"])\n",
        "\n",
        "        dW = deleta2.dot(store[\"Z\"].T)\n",
        "        db = np.sum(deleta2, axis=1, keepdims=True)\n",
        "\n",
        "        derivatives[\"dW2\"] = dW\n",
        "        derivatives[\"db2\"] = db\n",
        "\n",
        "        dAPrev = store[\"W2\"].T.dot(deleta2)\n",
        "        deleta1 = dAPrev * sigmoid_derivative(store[\"S1\"])\n",
        "\n",
        "        dW = deleta1.dot(store[\"X\"].T)\n",
        "        db = np.sum(deleta1, axis=1, keepdims=True)\n",
        "\n",
        "        derivatives[\"dW1\"] = dW\n",
        "        derivatives[\"db1\"] = db\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##############################################\n",
        " \n",
        "        return derivatives\n",
        " \n",
        "    def fit(self, X, Y, learning_rate=0.01, n_iterations=2500):\n",
        "        np.random.seed(1)\n",
        " \n",
        "        self.n = X.shape[0]\n",
        "        self.layers_size.insert(0, X.shape[1])\n",
        "        self.initialize_parameters()\n",
        "\n",
        "        for loop in range(n_iterations):\n",
        "            H, store = self.forward(X)\n",
        "            cost = -np.mean(Y * np.log(H.T+ 1e-8))\n",
        "            derivatives = self.backward(X, Y, store)\n",
        " \n",
        "            for l in range(1, self.L + 1):\n",
        "                # gradient descent\n",
        "                ########### complete the code ################\n",
        "                self.parameters[\"W2\"] = self.parameters[\"W2\"] - learning_rate * self.derivatives[\"dW2\"]\n",
        "                self.parameters[\"b2\"] = self.parameters[\"b2\"] - learning_rate * self.derivatives[\"db2\"]\n",
        "\n",
        "                self.parameters[\"W1\"] = self.parameters[\"W1\"] - learning_rate * self.derivatives[\"dW1\"]\n",
        "                self.parameters[\"b1\"] = self.parameters[\"b1\"] - learning_rate * self.derivatives[\"db1\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                ##############################################\n",
        "\n",
        "            if loop % 10 == 0:\n",
        "                print(\"[%3d/%3d] Cost: %.4f\"%(loop, n_iterations, cost),\n",
        "                      \"Train Accuracy: %2.2f\"%(self.predict(X, Y)))\n",
        " \n",
        "            self.costs.append(cost)\n",
        "            H, store = self.forward(X_test)\n",
        "            val_cost = -np.mean(y_test * np.log(H.T+ 1e-8))            \n",
        "            self.val_costs.append(val_cost)\n",
        " \n",
        "    def predict(self, X, Y):\n",
        "        A, cache = self.forward(X)\n",
        "        y_hat = np.argmax(A, axis=0)\n",
        "        Y = np.argmax(Y, axis=1)\n",
        "        accuracy = (y_hat == Y).mean()\n",
        "        return accuracy * 100\n",
        " \n",
        "    def plot_cost(self):\n",
        "        plt.figure()\n",
        "        plt.plot(np.arange(len(self.costs)), self.costs)\n",
        "        plt.plot(np.arange(len(self.val_costs)), self.val_costs)\n",
        "        plt.xlabel(\"epochs\")\n",
        "        plt.ylabel(\"cost\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yTl-FzF9Jqg"
      },
      "source": [
        "Build Model with 50 number of hidden units and train it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "_ta25z2B5Cm0",
        "outputId": "697b4425-0cfb-4f61-9068-0156704db562"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-0dae6039df11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mann\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mANN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Accuracy: %.4f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Accuracy: %.4f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-83e6f659b542>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, learning_rate, n_iterations)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mderivatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-83e6f659b542>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, X, Y, store)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mderivatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"H\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mdeleta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"S2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (50,100) (10,100) "
          ]
        }
      ],
      "source": [
        "layers_dims = [50, 10]\n",
        "\n",
        "ann = ANN(layers_dims)\n",
        "ann.fit(X_train, y_train, learning_rate=.01, n_iterations=200)\n",
        "print(\"Train Accuracy: %.4f\" % ann.predict(X_train, y_train))\n",
        "print(\"Test Accuracy: %.4f\" % ann.predict(X_test, y_test))\n",
        "ann.plot_cost()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Lab 3.ipynb",
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}